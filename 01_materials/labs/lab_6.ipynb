{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using Neural Networks\n",
    "\n",
    "The goal of this notebook is to learn to use Neural Networks for text classification.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Train a shallow model with learning embeddings\n",
    "- Download pre-trained embeddings from Glove\n",
    "- Use these pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BBC topic classification dataset\n",
    "\n",
    "The BBC provides some benchmark topic classification datasets in English at: http://mlg.ucd.ie/datasets/bbc.html.\n",
    "\n",
    "The raw text (encoded with the latin-1 character encoding) of the news can be downloaded as a ZIP archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "BBC_DATASET_URL = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\"\n",
    "zip_filename = BBC_DATASET_URL.rsplit('/', 1)[1]\n",
    "BBC_DATASET_FOLDER = 'bbc'\n",
    "if not op.exists(zip_filename):\n",
    "    print(\"Downloading %s to %s...\" % (BBC_DATASET_URL, zip_filename))\n",
    "    urlretrieve(BBC_DATASET_URL, zip_filename)\n",
    "\n",
    "if not op.exists(BBC_DATASET_FOLDER):\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as f:\n",
    "        print(\"Extracting contents of %s...\" % zip_filename)\n",
    "        f.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the five folders contains text files from one of the five topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = sorted(folder for folder in os.listdir(BBC_DATASET_FOLDER)\n",
    "                      if op.isdir(op.join(BBC_DATASET_FOLDER, folder)))\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
      "\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "\n",
      "Time ...\n"
     ]
    }
   ],
   "source": [
    "# Example of a file in the \"business\" category\n",
    "with open(op.join(BBC_DATASET_FOLDER, 'business', '001.txt'), 'rb') as f:\n",
    "    print(f.read().decode('latin-1')[:500] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly partition the text files in a training and test set while recording the target category of each file as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = []\n",
    "filenames = []\n",
    "for target_id, target_name in enumerate(target_names):\n",
    "    class_path = op.join(BBC_DATASET_FOLDER, target_name) # e.g. 'bbc/business'\n",
    "    for filename in sorted(os.listdir(class_path)):\n",
    "        filenames.append(op.join(class_path, filename))\n",
    "        target.append(target_id)\n",
    "\n",
    "target = np.asarray(target, dtype=np.int32)\n",
    "target_train, target_test, filenames_train, filenames_test = train_test_split(\n",
    "    target, filenames, test_size=200, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What we now have is pairs of target labels (which category the document belongs to) and filenames (where the document is stored on disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 3, 4, 3]),\n",
       " ['bbc\\\\business\\\\475.txt',\n",
       "  'bbc\\\\entertainment\\\\152.txt',\n",
       "  'bbc\\\\sport\\\\127.txt',\n",
       "  'bbc\\\\tech\\\\095.txt',\n",
       "  'bbc\\\\sport\\\\464.txt'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train[:5], filenames_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4.582 MB\n"
     ]
    }
   ],
   "source": [
    "size_in_bytes = sum([len(open(fn, 'rb').read()) for fn in filenames_train])\n",
    "print(\"Training set size: %0.3f MB\" % (size_in_bytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is small so we can load everything into memory right now (which simplifies our code later). If we had substantially more data, we would need to use a `tf.data.Dataset` to stream it from disk in batches during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_train]\n",
    "texts_test = [open(fn, 'rb').read().decode('latin-1') for fn in filenames_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first baseline model\n",
    "\n",
    "For simple topic classification problems, one should always try a simple method first. Let's try using a `CountVectorizer` followed by `LogisticRegression` as a baseline. What this will do is:\n",
    "\n",
    "- Convert the text documents to a matrix of token counts (each row is a document, each column is a word, each cell is the count of the word in the document)\n",
    "- Train a logistic regression model on this matrix\n",
    "\n",
    "It's a very efficient method and should give us a strong baseline to compare our deep learning method against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of the first document:\n",
      "Watchdog probes Vivendi bond sale\n",
      "\n",
      "French stock market regulator AMF has filed complaints against me...\n",
      "----------\n",
      "Sampling of vocabulary counts in the document:\n",
      "bank 2\n",
      "bankruptcy 0\n",
      "banks 0\n",
      "barcelona 0\n",
      "based 0\n",
      "basic 0\n",
      "basis 1\n",
      "bath 0\n",
      "battle 0\n",
      "bbc 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Understanding what the CountVectorizer does\n",
    "vectorizer = CountVectorizer(max_features=2000) # only keep the 2000 most frequent words\n",
    "X_train = vectorizer.fit_transform(texts_train)\n",
    "\n",
    "# Compare the content of the first document with the vocabulary\n",
    "print(\"Start of the first document:\")\n",
    "print(texts_train[0][0:100] + '...')\n",
    "print('----------')\n",
    "print(\"Sampling of vocabulary counts in the document:\")\n",
    "for word, count in zip(vectorizer.get_feature_names_out()[200:210], X_train.toarray()[0][200:210]):\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "text_classifier = make_pipeline(\n",
    "    CountVectorizer(max_features=2000),\n",
    "    LogisticRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 859 ms\n",
      "Wall time: 1.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarit\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%time _ = text_classifier.fit(texts_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You may get a warning above that \"lbfgs failed to converge\". This means that the optimization algorithm did not reach the desired precision. This is not a big deal here, as we are not looking for the best possible accuracy, but just a baseline. We can check the accuracy of this model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.score(texts_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 95 percent testing accuracy on a very simple baseline. It's quite unlikely that we can significantly beat that baseline with a more complex deep learning based model. This is simply not a complex task - we wouldn't expect to see this level of performance from a simple model on a real-world text classification problem. Let's move on, and see how well we can do with a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text for the (supervised) CBOW model\n",
    "\n",
    "We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n",
    "\n",
    "The following cells uses Keras to preprocess text:\n",
    "- using a tokenizer. This converts the texts into sequences of indices representing the `20000` most frequent words\n",
    "- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`). For example, if we were padding to three words, and we had a sequence of just \"dog\", we would pad it to \"[\\<dog token\\>,0,0]\".\n",
    "- we convert the output classes as 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30995 unique tokens.\n",
      "Example of word_index: [('the', 1), ('to', 2), ('of', 3), ('and', 4), ('a', 5)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(f'Example of word_index: {list(word_index.items())[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenized sequences are converted to list of token ids (with an integer code). We can convert them back to text to see what they now look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:          ['Watchdog', 'probes', 'Vivendi', 'bond']\n",
      "Tokenized text:         [1857, 9454, 5251, 1973]\n",
      "Converted back to text: ['watchdog', 'probes', 'vivendi', 'bond']\n"
     ]
    }
   ],
   "source": [
    "print(f'Original text:          {texts_train[0].split(\" \")[0:4]}')\n",
    "print(f'Tokenized text:         {sequences[0][0:4]}')\n",
    "print(f'Converted back to text: {[index_to_word.get(i, \"UNK\") for i in sequences[0][0:4]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the tokenized sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length: 382.6\n",
      "max length: 4355\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while we do have sequences up to 4355 words long, the vast majority of sequences are less than 1000 words long. We can use this information to truncate or pad all the sequences to 1000 symbols to build the training set. This will simplify our model and speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2025, 1000)\n",
      "Shape of data test tensor: (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# Make all sequences exactly 1000 words long\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (2025, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(target_train)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple supervised CBOW model in Keras\n",
    "\n",
    "The following computes a very simple model, as described in [fastText](https://github.com/facebookresearch/fastText):\n",
    "\n",
    "<img src=\"images/fasttext.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "- Build an embedding layer mapping each word to a vector representation\n",
    "- Compute the vector representation of all words in each sequence and average them\n",
    "- Add a dense layer to output 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = len(target_names) \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True), # Just like we've seen in previous labs\n",
    "    GlobalAveragePooling1D(), # This layer averages the embeddings of all words in the sequence\n",
    "    Dense(N_CLASSES, activation='softmax') # This layer outputs a probability distribution over the 5 classes\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 2s 20ms/step - loss: 1.4055 - accuracy: 0.4885 - val_loss: 1.0655 - val_accuracy: 0.8276\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.6970 - accuracy: 0.8919 - val_loss: 0.4472 - val_accuracy: 0.9409\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 1s 13ms/step - loss: 0.2714 - accuracy: 0.9775 - val_loss: 0.2136 - val_accuracy: 0.9901\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.1257 - accuracy: 0.9929 - val_loss: 0.1469 - val_accuracy: 0.9754\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 1s 16ms/step - loss: 0.0695 - accuracy: 0.9967 - val_loss: 0.1142 - val_accuracy: 0.9803\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0431 - accuracy: 0.9973 - val_loss: 0.0990 - val_accuracy: 0.9754\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 0.9754\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9803\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9803\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4b4ecf160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "- Evaluate the model on the test set\n",
    "- Identify an example of a mis-classified document and display the text of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.97\n",
      "6\n",
      "Sport betting rules in spotlight\n",
      "\n",
      "A group of MPs and peers has called for a tightening of regulations controlling betting on sport.\n",
      "\n",
      "The Parliamentary Group on Betting and Gaming held a substantial inquiry into betting last year. It followed fears that a massive increase in betting on sport, such as that done using the internet and mobile phones, has led to more cheating. The all-party group recommended 15 ways to protect punters and improve the integrity of sports betting. They include a proposal for raising the maximum jail sentence for gambling cheats above the current two years. Lord Condon, head of the International Cricket Council's anti-corruption unit, who originally made the call for longer prison sentences, said the two-year penalty was \"derisory\". \"You could get a bigger sentence for failing to pay your hotel bill criminally than you could for corruption in major sports. \"Symbolically, a higher penalty, perhaps as the Bill passes through the two Houses, might be appropriate.\"\n",
      "\n",
      "The report recommended the governing bodies of sports have a say in the type of bets offered to punters, and for bookmakers to set up \"audit trails\" - something the new betting exchanges already do - to allow suspicious betting patterns to be traced.\n",
      "\n",
      "Lord Faulkner of Worcester, who chaired the inquiry, said: \"Whilst we accept that the greater part of sports betting is neither corrupt nor unfair to punters, the evidence convinces us that the growth of betting exchanges - because of the facility they provide to bet against a result - has increased the potential for corruption. \"It is important that the government works with sporting administrators to review the difficulties faced by governing bodies in convicting the guilty and penalising them appropriately.\" The panel's aim was to try to define what constitutes cheating, assess how much might be going on and suggest what the government might do to put it right. As well as the growth of internet and mobile phone betting, there has been the creation of betting exchanges which allow punters to fix odds between themselves. Betting exchanges allow punters to back (to win) but also lay (to lose) a horse. This means they can control their odds at winning by placing their money both ways.\n",
      "\n",
      "business\n",
      "politics\n"
     ]
    }
   ],
   "source": [
    "output_test = model(x_test)\n",
    "test_cases = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy\",np.mean(test_cases ==target_test))\n",
    "\n",
    "# Your code here\n",
    "misclassified = np.where(test_cases != target_test)[0]\n",
    "print(len(misclassified))\n",
    "print(texts_test[misclassified[0]])\n",
    "print(target_names[test_cases[misclassified[0]]])\n",
    "print(target_names[target_test[misclassified[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building more complex models\n",
    "\n",
    "**Exercise**\n",
    "- Copy the previous model, and add more complexity to it. You can try adding more layers, or using a different type of layer (e.g. LSTM, Conv1D, etc.)\n",
    "- Some examples of what you could do:\n",
    "    - Add a LSTM layer before the dense layer ([LSTM documentation](https://keras.io/layers/recurrent/#lstm))\n",
    "    - Add a Conv1D layer after the embedding layer ([Conv1D documentation](https://keras.io/layers/convolutional/#conv1d))\n",
    "    - Add more dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    # Your code here\n",
    "    Embedding(input_dim =MAX_NB_WORDS,output_dim=EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH, trainable=True),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    LSTM(100),\n",
    "    Flatten(),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "57/57 [==============================] - 12s 154ms/step - loss: 1.2064 - accuracy: 0.4424 - val_loss: 0.8926 - val_accuracy: 0.6108\n",
      "Epoch 2/15\n",
      "57/57 [==============================] - 7s 130ms/step - loss: 0.6790 - accuracy: 0.7124 - val_loss: 0.6632 - val_accuracy: 0.7192\n",
      "Epoch 3/15\n",
      "57/57 [==============================] - 7s 121ms/step - loss: 0.4815 - accuracy: 0.8101 - val_loss: 0.5796 - val_accuracy: 0.7635\n",
      "Epoch 4/15\n",
      "57/57 [==============================] - 6s 110ms/step - loss: 0.3185 - accuracy: 0.8880 - val_loss: 0.5655 - val_accuracy: 0.7882\n",
      "Epoch 5/15\n",
      "57/57 [==============================] - 9s 156ms/step - loss: 0.2233 - accuracy: 0.9193 - val_loss: 0.8550 - val_accuracy: 0.7340\n",
      "Epoch 6/15\n",
      "57/57 [==============================] - 10s 169ms/step - loss: 0.1983 - accuracy: 0.9286 - val_loss: 0.7188 - val_accuracy: 0.8177\n",
      "Epoch 7/15\n",
      "57/57 [==============================] - 8s 141ms/step - loss: 0.0949 - accuracy: 0.9654 - val_loss: 0.6211 - val_accuracy: 0.8374\n",
      "Epoch 8/15\n",
      "57/57 [==============================] - 9s 151ms/step - loss: 0.0364 - accuracy: 0.9901 - val_loss: 0.8382 - val_accuracy: 0.8030\n",
      "Epoch 9/15\n",
      "57/57 [==============================] - 13s 228ms/step - loss: 0.0614 - accuracy: 0.9819 - val_loss: 0.7495 - val_accuracy: 0.8325\n",
      "Epoch 10/15\n",
      "57/57 [==============================] - 26s 466ms/step - loss: 0.0220 - accuracy: 0.9956 - val_loss: 0.8429 - val_accuracy: 0.8276\n",
      "Epoch 11/15\n",
      "57/57 [==============================] - 22s 380ms/step - loss: 0.0185 - accuracy: 0.9951 - val_loss: 0.8420 - val_accuracy: 0.8325\n",
      "Epoch 12/15\n",
      "57/57 [==============================] - 22s 379ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.8676 - val_accuracy: 0.8177\n",
      "Epoch 13/15\n",
      "57/57 [==============================] - 25s 434ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8341 - val_accuracy: 0.8374\n",
      "Epoch 14/15\n",
      "57/57 [==============================] - 22s 364ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8434 - val_accuracy: 0.8276\n",
      "Epoch 15/15\n",
      "57/57 [==============================] - 20s 349ms/step - loss: 8.1914e-04 - accuracy: 1.0000 - val_loss: 0.8545 - val_accuracy: 0.8276\n",
      "Test accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)\n",
    "\n",
    "output_test = model(x_test)\n",
    "test_casses = np.argmax(output_test, axis=-1)\n",
    "print(\"Test accuracy:\", np.mean(test_casses == target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained embeddings\n",
    "\n",
    "The file `glove100K.100d.txt` is an extract of [Glove](http://nlp.stanford.edu/projects/glove/) Vectors, that were trained on English Wikipedia and the Gigaword 5 corpus. They differ from word2vec in the way the vectors are trained, but the idea is the same: each word is represented as a vector of `100` numbers.\n",
    "\n",
    "We extracted the `100 000` most frequent words for you, and the code below downloads them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://github.com/m2dsupsdlclass/lectures-labs/releases/download/0.3/glove100k.100d.zip to glove100k.100d.zip...\n",
      "extracting glove100k.100d.zip...\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained Glove Word2Vec\n",
    "URL_REPRESENTATIONS = \"https://github.com/m2dsupsdlclass/lectures-labs/releases/download/0.3/glove100k.100d.zip\"\n",
    "ZIP_REPRESENTATIONS = \"glove100k.100d.zip\"\n",
    "FILE_REPRESENTATIONS = \"glove100K.100d.txt\"\n",
    "\n",
    "if not op.exists(ZIP_REPRESENTATIONS):\n",
    "    print('Downloading from %s to %s...' % (URL_REPRESENTATIONS, ZIP_REPRESENTATIONS))\n",
    "    urlretrieve(URL_REPRESENTATIONS, './' + ZIP_REPRESENTATIONS)\n",
    "\n",
    "if not op.exists(FILE_REPRESENTATIONS):\n",
    "    print(\"extracting %s...\" % ZIP_REPRESENTATIONS)\n",
    "    myzip = zipfile.ZipFile(ZIP_REPRESENTATIONS)\n",
    "    myzip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100000 different words in the file\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_vectors = []\n",
    "with open('glove100K.100d.txt', 'rb') as f:\n",
    "    word_idx = 0\n",
    "    for line in f:\n",
    "        values = line.decode('utf-8').split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = word_idx\n",
    "        embeddings_vectors.append(vector)\n",
    "        word_idx = word_idx + 1\n",
    "\n",
    "inv_index = {v: k for k, v in embeddings_index.items()}\n",
    "print(\"found %d different words in the file\" % word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Stack all embeddings in a large numpy array\n",
    "glove_embeddings = np.vstack(embeddings_vectors)\n",
    "glove_norms = np.linalg.norm(glove_embeddings, axis=-1, keepdims=True)\n",
    "glove_embeddings_normed = glove_embeddings / glove_norms\n",
    "print(glove_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings[idx]\n",
    "\n",
    "    \n",
    "def get_normed_emb(word):\n",
    "    idx = embeddings_index.get(word)\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        return glove_embeddings_normed[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6298e-01,  3.0141e-01,  5.7978e-01,  6.6548e-02,  4.5835e-01,\n",
       "       -1.5329e-01,  4.3258e-01, -8.9215e-01,  5.7747e-01,  3.6375e-01,\n",
       "        5.6524e-01, -5.6281e-01,  3.5659e-01, -3.6096e-01, -9.9662e-02,\n",
       "        5.2753e-01,  3.8839e-01,  9.6185e-01,  1.8841e-01,  3.0741e-01,\n",
       "       -8.7842e-01, -3.2442e-01,  1.1202e+00,  7.5126e-02,  4.2661e-01,\n",
       "       -6.0651e-01, -1.3893e-01,  4.7862e-02, -4.5158e-01,  9.3723e-02,\n",
       "        1.7463e-01,  1.0962e+00, -1.0044e+00,  6.3889e-02,  3.8002e-01,\n",
       "        2.1109e-01, -6.6247e-01, -4.0736e-01,  8.9442e-01, -6.0974e-01,\n",
       "       -1.8577e-01, -1.9913e-01, -6.9226e-01, -3.1806e-01, -7.8565e-01,\n",
       "        2.3831e-01,  1.2992e-01,  8.7721e-02,  4.3205e-01, -2.2662e-01,\n",
       "        3.1549e-01, -3.1748e-01, -2.4632e-03,  1.6615e-01,  4.2358e-01,\n",
       "       -1.8087e+00, -3.6699e-01,  2.3949e-01,  2.5458e+00,  3.6111e-01,\n",
       "        3.9486e-02,  4.8607e-01, -3.6974e-01,  5.7282e-02, -4.9317e-01,\n",
       "        2.2765e-01,  7.9966e-01,  2.1428e-01,  6.9811e-01,  1.1262e+00,\n",
       "       -1.3526e-01,  7.1972e-01, -9.9605e-04, -2.6842e-01, -8.3038e-01,\n",
       "        2.1780e-01,  3.4355e-01,  3.7731e-01, -4.0251e-01,  3.3124e-01,\n",
       "        1.2576e+00, -2.7196e-01, -8.6093e-01,  9.0053e-02, -2.4876e+00,\n",
       "        4.5200e-01,  6.6945e-01, -5.4648e-01, -1.0324e-01, -1.6979e-01,\n",
       "        5.9437e-01,  1.1280e+00,  7.5755e-01, -5.9160e-02,  1.5152e-01,\n",
       "       -2.8388e-01,  4.9452e-01, -9.1703e-01,  9.1289e-01, -3.0927e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_emb(\"computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most similar words\n",
    "\n",
    "Here we define a function to find the most similar words to a given word. The similarity is computed using the cosine similarity between the word embeddings. It can also accept multiple words, and it will take the average of the embeddings of the words to find the most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(words, topn=10):\n",
    "    query_emb = 0\n",
    "    # If we have a list of words instead of one word\n",
    "    # (bonus question)\n",
    "    if type(words) == list:\n",
    "        for word in words:\n",
    "            query_emb += get_emb(word)\n",
    "    else:\n",
    "        query_emb = get_emb(words)\n",
    "\n",
    "    query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "\n",
    "    # Large numpy vector with all cosine similarities\n",
    "    # between emb and all other words\n",
    "    cosines = np.dot(glove_embeddings_normed, query_emb)\n",
    "\n",
    "    # topn most similar indexes corresponding to cosines\n",
    "    idxs = np.argsort(cosines)[::-1][:topn]\n",
    "\n",
    "    # pretty return with word and similarity\n",
    "    return [(inv_index[idx], cosines[idx]) for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cpu', 0.9999999),\n",
       " ('processor', 0.7793438),\n",
       " ('cpus', 0.7651843),\n",
       " ('microprocessor', 0.7360635),\n",
       " ('processors', 0.67348146),\n",
       " ('motherboard', 0.6675773),\n",
       " ('x86', 0.6655922),\n",
       " ('pentium', 0.64758503),\n",
       " ('gpu', 0.6448882),\n",
       " ('i/o', 0.6352353)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"nvidia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus: sum of two word embeddings\n",
    "most_similar([\"toronto\", \"leaf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Displaying vectors with  t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "word_emb_tsne = TSNE(perplexity=30).fit_transform(glove_embeddings_normed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "axis = plt.gca()\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.scatter(word_emb_tsne[:, 0], word_emb_tsne[:, 1], marker=\".\", s=1)\n",
    "\n",
    "for idx in range(1000):\n",
    "    plt.annotate(inv_index[idx],\n",
    "                 xy=(word_emb_tsne[idx, 0], word_emb_tsne[idx, 1]),\n",
    "                 xytext=(0, 0), textcoords='offset points')\n",
    "plt.savefig(\"tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained embeddings in our model\n",
    "\n",
    "We want to use these pre-trained embeddings for transfer learning. This process is rather similar than transfer learning in image recognition: the features learnt on words might help us bootstrap the learning process, and increase performance if we don't have enough training data.\n",
    "\n",
    "- We initialize embedding matrix from the model with Glove embeddings:\n",
    " - take all unique words from our BBC news dataset to build a vocabulary (`MAX_NB_WORDS = 20000`), and look up their Glove embedding \n",
    " - place the Glove embedding at the corresponding index in the matrix\n",
    " - if the word is not in the Glove vocabulary, we only place zeros in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words_in_matrix = 0\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = get_emb(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector # Place the Glove embedding at the corresponding index in the matrix\n",
    "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
    "        \n",
    "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a layer with pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_layer = Embedding(\n",
    "    MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model with pre-trained Embeddings\n",
    "\n",
    "Now we can build a model with pre-trained embeddings. We will use the same architecture as before, but we will use the `pretrained_embedding_layer` as the first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Add the pre-defined and pre-trained embedding layer\n",
    "    pretrained_embedding_layer,\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(N_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Set the embedding layer's trainable attribute to False to not fine-tune the embeddings - you can try to change this\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reality check\n",
    "\n",
    "On small/medium datasets (few 10,000s) of reasonably large documents (e.g. more than a few paragraphs), simpler classification methods usually perform better, and are much more efficient to train and use. Here are two resources to go further, if you are curious:\n",
    "- Naive Bayes approach, using scikit-learn [http://scikit-learn.org/stable/datasets/twenty_newsgroups.html](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "- Alec Radford (OpenAI) gave a very interesting presentation, showing that you need a VERY large dataset to have real gains from GRU/LSTM in text classification [https://www.slideshare.net/odsc/alec-radfordodsc-presentation](https://www.slideshare.net/odsc/alec-radfordodsc-presentation)\n",
    "\n",
    "Training deep architectures from random init on text classification is usually a waste of time.\n",
    "\n",
    "However, when looking at features, one can see that classification using simple methods isn't very robust, and won't generalize well to slightly different domains (e.g. forum posts => emails)\n",
    "\n",
    "Nowadays, the strategy would be to use pre-trained deep network (BERT) to extract features and fit a linear classifer on top of this. This is especially useful when classifying short texts (e.g. one or a few sentences) as this kind of tasks can be very sensitive to understanding the meaning resulting from intra-sentence interactions between words. The next session on attentional mechanisms and pre-trained transformer-based word models will explain this in more details.\n",
    "\n",
    "### Bonus Task (Optional)\n",
    "\n",
    "- The `Transformers` library from Hugging Face provides a very easy way to use pre-trained models for text classification. You can try to use it to classify the BBC dataset. Part of the code needed is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "inputs = tokenizer(texts_train, return_tensors=\"tf\", padding=True, truncation=True, max_length=100)\n",
    "model_inputs = {key: val for key, val in inputs.items()}\n",
    "\n",
    "# Fine-tune the model: the model is in the same format as our Keras models, so you can use the same methods to train it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
